{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from turtle import distance\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "import numpy as np # this module is useful to work with numerical arrays\n",
    "import pandas as pd \n",
    "import random \n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader,random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anagara8/.conda/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class KalmanFilter(object):\n",
    "    def __init__(self, dt, u_x,u_y, std_acc, x_std_meas, y_std_meas):\n",
    "        \"\"\"\n",
    "        :param dt: sampling time (time for 1 cycle)\n",
    "        :param u_x: acceleration in x-direction\n",
    "        :param u_y: acceleration in y-direction\n",
    "        :param std_acc: process noise magnitude\n",
    "        :param x_std_meas: standard deviation of the measurement in x-direction\n",
    "        :param y_std_meas: standard deviation of the measurement in y-direction\n",
    "        \"\"\"\n",
    "        # Define sampling time\n",
    "        self.dt = dt\n",
    "        # Define the  control input variables\n",
    "        self.u = np.matrix([[u_x],[u_y]])\n",
    "        # Intial State\n",
    "        self.x = np.matrix([[0], [0], [0], [0]])\n",
    "        # Define the State Transition Matrix A\n",
    "        self.A = np.matrix([[1, 0, self.dt, 0],\n",
    "                            [0, 1, 0, self.dt],\n",
    "                            [0, 0, 1, 0],\n",
    "                            [0, 0, 0, 1]])\n",
    "        # Define the Control Input Matrix B\n",
    "        self.B = np.matrix([[(self.dt**2)/2, 0],\n",
    "                            [0, (self.dt**2)/2],\n",
    "                            [self.dt,0],\n",
    "                            [0,self.dt]])\n",
    "        # Define Measurement Mapping Matrix\n",
    "        self.H = np.matrix([[1, 0, 0, 0],\n",
    "                            [0, 1, 0, 0]])\n",
    "        #Initial Process Noise Covariance\n",
    "        self.Q = np.matrix([[(self.dt**4)/4, 0, (self.dt**3)/2, 0],\n",
    "                            [0, (self.dt**4)/4, 0, (self.dt**3)/2],\n",
    "                            [(self.dt**3)/2, 0, self.dt**2, 0],\n",
    "                            [0, (self.dt**3)/2, 0, self.dt**2]]) * std_acc**2\n",
    "        #Initial Measurement Noise Covariance\n",
    "        self.R = np.matrix([[x_std_meas**2,0],\n",
    "                           [0, y_std_meas**2]])\n",
    "        #Initial Covariance Matrix\n",
    "        self.P = np.eye(self.A.shape[1])\n",
    "\n",
    "    def predict(self):\n",
    "        # Refer to :Eq.(9) and Eq.(10)  in https://machinelearningspace.com/object-tracking-simple-implementation-of-kalman-filter-in-python/?preview_id=1364&preview_nonce=52f6f1262e&preview=true&_thumbnail_id=1795\n",
    "        # Update time state\n",
    "        #x_k =Ax_(k-1) + Bu_(k-1)     Eq.(9)\n",
    "        self.x = np.dot(self.A, self.x) + np.dot(self.B, self.u)\n",
    "        # Calculate error covariance\n",
    "        # P= A*P*A' + Q               Eq.(10)\n",
    "        self.P = np.dot(np.dot(self.A, self.P), self.A.T) + self.Q\n",
    "        return self.x[0:2]\n",
    "\n",
    "    def update(self, z):\n",
    "        # Refer to :Eq.(11), Eq.(12) and Eq.(13)  in https://machinelearningspace.com/object-tracking-simple-implementation-of-kalman-filter-in-python/?preview_id=1364&preview_nonce=52f6f1262e&preview=true&_thumbnail_id=1795\n",
    "        # S = H*P*H'+R\n",
    "        S = np.dot(self.H, np.dot(self.P, self.H.T)) + self.R\n",
    "        # Calculate the Kalman Gain\n",
    "        # K = P * H'* inv(H*P*H'+R)\n",
    "        K = np.dot(np.dot(self.P, self.H.T), np.linalg.inv(S))  #Eq.(11)\n",
    "        self.x = np.round(self.x + np.dot(K, (z - np.dot(self.H, self.x))))   #Eq.(12)\n",
    "        I = np.eye(self.H.shape[1])\n",
    "        # Update error covariance matrix\n",
    "        self.P = (I - (K * self.H)) * self.P   #Eq.(13)\n",
    "        return self.x[0:2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def detect(frame,debugMode):\n",
    "    # Convert frame from BGR to GRAY\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    if (debugMode):\n",
    "        cv2.imshow('gray', gray)\n",
    "    # Edge detection using Canny function\n",
    "    img_edges = cv2.Canny(gray,  10, 100, 3)\n",
    "    if (debugMode):\n",
    "        cv2.imshow('img_edges', img_edges)\n",
    "    # Convert to black and white image\n",
    "    ret, img_thresh = cv2.threshold(img_edges, 254, 255,cv2.THRESH_BINARY)\n",
    "    if (debugMode):\n",
    "        cv2.imshow('img_thresh', img_thresh)\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(img_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Set the accepted minimum & maximum radius of a detected object\n",
    "    min_radius_thresh= 3\n",
    "    max_radius_thresh= 30   \n",
    "    centers=[]\n",
    "    for c in contours:\n",
    "        # ref: https://docs.opencv.org/trunk/dd/d49/tutorial_py_contour_features.html\n",
    "        (x, y), radius = cv2.minEnclosingCircle(c)\n",
    "        radius = int(radius)\n",
    "        #Take only the valid circles\n",
    "        if (radius > min_radius_thresh) and (radius < max_radius_thresh):\n",
    "            centers.append(np.array([[x], [y]]))\n",
    "    cv2.imshow('contours', img_thresh)\n",
    "    return centers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#KalmanFilter(dt, u_x, u_y, std_acc, x_std_meas, y_std_meas)\n",
    "KF = KalmanFilter(0.1, 1, 1, 1, 0.1,0.1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "HiSpeed= 100\n",
    "ControlSpeedVar = 30  #Lowest: 1 - Highest:100\n",
    "debugMode=1\n",
    "#Create KalmanFilter object KF\n",
    "#KalmanFilter(dt, u_x, u_y, std_acc, x_std_meas, y_std_meas)\n",
    "KF = KalmanFilter(0.1, 1, 1, 1, 0.1,0.1)\n",
    "# Create opencv video capture object\n",
    "VideoCap = cv2.VideoCapture('Videos/video1_Top.mp4')\n",
    "while(True):\n",
    "    # Read frame\n",
    "    ret, frame = VideoCap.read()\n",
    "    # Detect object\n",
    "    centers = detect(frame,debugMode)\n",
    "    \n",
    "    # If centroids are detected then track them\n",
    "    if (len(centers) > 0):\n",
    "        # Draw the detected circle\n",
    "        cv2.circle(frame, (int(centers[0][0]), int(centers[0][1])), 10, (0, 191, 255), 2)\n",
    "        # Predict\n",
    "        (x, y) = KF.predict()\n",
    "        # Draw a rectangle as the predicted object position\n",
    "        cv2.rectangle(frame, (x - 15, y - 15), (x + 15, y + 15), (255, 0, 0), 2)\n",
    "        # Update\n",
    "        (x1, y1) = KF.update(centers[0])\n",
    "        # Draw a rectangle as the estimated object position\n",
    "        cv2.rectangle(frame, (x1 - 15, y1 - 15), (x1 + 15, y1 + 15), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"Estimated Position\", (x1 + 15, y1 + 10), 0, 0.5, (0, 0, 255), 2)\n",
    "        cv2.putText(frame, \"Predicted Position\", (x + 15, y), 0, 0.5, (255, 0, 0), 2)\n",
    "        cv2.putText(frame, \"Measured Position\", (centers[0][0] + 15, centers[0][1] - 15), 0, 0.5, (0,191,255), 2)\n",
    "    cv2.imshow('image', frame)\n",
    "    if cv2.waitKey(2) & 0xFF == ord('q'):\n",
    "        VideoCap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    cv2.waitKey(HiSpeed-ControlSpeedVar+1)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/home/anagara8/.vscode/extensions/ms-toolsai.jupyter-2021.8.1054968649/out/client/extension.js:90:320068)",
      "at w.execute (/home/anagara8/.vscode/extensions/ms-toolsai.jupyter-2021.8.1054968649/out/client/extension.js:90:319389)",
      "at w.start (/home/anagara8/.vscode/extensions/ms-toolsai.jupyter-2021.8.1054968649/out/client/extension.js:90:315205)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/anagara8/.vscode/extensions/ms-toolsai.jupyter-2021.8.1054968649/out/client/extension.js:90:329732)",
      "at async t.CellExecutionQueue.start (/home/anagara8/.vscode/extensions/ms-toolsai.jupyter-2021.8.1054968649/out/client/extension.js:90:329272)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.10 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "9e25aae5d5ec30c1275ce65064366f5e889edfac406ad25a454b13c66848219b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}