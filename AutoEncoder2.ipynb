{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f8fb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9610a3a",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26646e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import distance\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "import numpy as np # this module is useful to work with numerical arrays\n",
    "import pandas as pd \n",
    "import random \n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader,random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba50d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "484e7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/media/data_cifs/anagara8/video_processed_frames/'\n",
    "\n",
    "img_list = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "331c1e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "618210"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5a9e6e",
   "metadata": {},
   "source": [
    "### WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a0685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking training\n",
    "import wandb\n",
    "\n",
    "# wandb.init(project=\"nih_animal_behavior\", entity=\"serrelab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740ef918",
   "metadata": {},
   "source": [
    "### Processing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d46ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'dataset'\n",
    "number_of_frames_per_video = 2\n",
    "real_train_data = []\n",
    "dimension = (256, 256)\n",
    "\n",
    "train_dataset_list = pickle.load(open('/home/anagara8/Documents/Autoencoder/postext_frames.pickle', 'rb'))\n",
    "\n",
    "\n",
    "for data in train_dataset_list:\n",
    "    temp = []\n",
    "    if len(data) == number_of_frames_per_video:\n",
    "        for frame_index in range(number_of_frames_per_video):\n",
    "            temp.append(cv2.resize(cv2.cvtColor(data[frame_index], cv2.COLOR_BGR2GRAY), dimension, interpolation = cv2.INTER_AREA))\n",
    "    \n",
    "    else:\n",
    "        continue\n",
    "    real_train_data += temp\n",
    "    \n",
    "# Same data for training and testing\n",
    "train_dataset_list = real_train_data\n",
    "test_dataset_list = real_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9df359",
   "metadata": {},
   "source": [
    "#### Converting to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df967a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_tensor = torch.Tensor(train_dataset_list)\n",
    "testing_data_tensor = torch.Tensor(test_dataset_list)\n",
    "print(\"Converted to Tensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd04e7ae",
   "metadata": {},
   "source": [
    "#### Converting to Tensor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a7847",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = TensorDataset(training_data_tensor)\n",
    "testing_dataset = TensorDataset(testing_data_tensor)\n",
    "print(\"Converted to Tensor Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b64c2",
   "metadata": {},
   "source": [
    "#### Print a sample of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3cf289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_dataset), \"x\", len(training_dataset[0]), \"x\", len(training_dataset[0][0]), \"x\", len(training_dataset[0][0][0]), \"x\", 1)\n",
    "for i in training_dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c3ddd",
   "metadata": {},
   "source": [
    "#### Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ee395",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=len(training_dataset)\n",
    "\n",
    "print(\"Length:\",m, \"-> Training set size:\", int(math.ceil(m-m*0.2)), \"| Validation set size:\", int(m*0.2))\n",
    "train_data, val_data = random_split(training_dataset, [int(math.ceil(m-m*0.2)), int(m*0.2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f2b374",
   "metadata": {},
   "source": [
    "#### Converting to DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8365178",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "train_set_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "valid_set_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
    "test_set_loader = torch.utils.data.DataLoader(testing_dataset, batch_size=batch_size,shuffle=True)\n",
    "print(\"Converted to DataLoader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb0b08",
   "metadata": {},
   "source": [
    "#### Looking at one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b7dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = enumerate(train_set_loader.dataset)\n",
    "batch_idx, example_data = next(examples)\n",
    "batch_idx, example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e33bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(256 * 256, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(True), nn.Linear(128, 64), nn.ReLU(True), nn.Linear(64, 32))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True), nn.Linear(256, 256 * 256), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = autoencoder().cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-02, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07463e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 256, 256)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_set_loader:\n",
    "        img = data[0]\n",
    "#         print(img)\n",
    "        img = img.view(img.size(0), -1)\n",
    "        img = Variable(img).cuda()\n",
    "        # ===================forward=====================\n",
    "        output = model(img)\n",
    "        loss = criterion(output, img)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, loss.data))\n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './mlp_img/image_{}.png'.format(epoch))\n",
    "\n",
    "torch.save(model.state_dict(), './sim_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b116478",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "max_epochs = 20\n",
    "outputs = train(model, num_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ed5351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
