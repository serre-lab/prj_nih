{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91209ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models,transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader,random_split\n",
    "from __future__ import print_function\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "from torch.autograd import Function\n",
    "from collections import OrderedDict\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torchvision.models as models\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import collections\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from sklearn.model_selection import KFold\n",
    "# from torchsummary import summary\n",
    "import glob\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import collections\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tokenizers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer, LineByLineTextDataset, BertModel, BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "SAVING_FRAMES_PER_SECOND = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700a2227",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FOLDER = \"/cifs/data/tserre/CLPS_Serre_Lab/nih/files_to_send/\"\n",
    "ACTION_FOLDER = \"/cifs/data/tserre/CLPS_Serre_Lab/nih/inference/results_csv/\"\n",
    "CONDENSED_ACTION_FOLDER = \"/cifs/data/tserre/CLPS_Serre_Lab/anagara8/prj_nih/Notebooks/CondensedActions/\"\n",
    "condensed_action_folders = os.listdir(CONDENSED_ACTION_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8682986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BEH_LABELS = ['drink', 'eat', 'groom', 'hang', 'sniff', 'rear', 'rest', 'walk', 'nibble']\n",
    "action_colours = ['tab:blue', 'tab:orange', 'limegreen', 'deepskyblue', 'tab:pink', 'tab:purple', 'tab:brown', 'red', 'black']\n",
    "len(action_colours) == len(BEH_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db168083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohort_search(cohort, search_root, ret=False):\n",
    "    cohort = str(cohort).upper()\n",
    "    all_folders = [folder for folder in os.listdir(search_root) if folder.startswith(\"FC-\"+cohort)]\n",
    "    all_folders.sort()\n",
    "    all_folder_list = \"\\n\".join(all_folders)\n",
    "    if ret:\n",
    "        return all_folders\n",
    "    else:\n",
    "        print(all_folder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7049478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mouse_videos(mouse_id, cohort, ret=False):\n",
    "    \n",
    "    search_for = \"action\"\n",
    "    \n",
    "    if search_for == \"videos\":\n",
    "        print(\"Looking for videos..\")\n",
    "        file_extension = \".mp4\" \n",
    "        ROOT_FOLDER = VIDEO_FOLDER\n",
    "    else:\n",
    "        print(\"Looking for actions...\")\n",
    "        file_extension = \".csv\"\n",
    "        ROOT_FOLDER = ACTION_FOLDER\n",
    "    \n",
    "    mouse_id = str(mouse_id)\n",
    "    cohort_folders = cohort_search(cohort, ROOT_FOLDER, ret=True)\n",
    "    \n",
    "    preexposure_folders = [folder for folder in cohort_folders if \"exp\" in folder]\n",
    "    postcond_folders = [folder for folder in cohort_folders if \"cond\" in folder]\n",
    "    postext_folders = [folder for folder in cohort_folders if \"xt\" in folder]\n",
    "    postret_folders = [folder for folder in cohort_folders if \"tre\" in folder]\n",
    "    \n",
    "    preexposure_videos = []\n",
    "    postcond_videos = []\n",
    "    postext_videos = []\n",
    "    postret_videos = []\n",
    "    \n",
    "    for folder in preexposure_folders:\n",
    "        preexposure_videos += [video+'/'+folder for video in os.listdir(ROOT_FOLDER+folder) if (mouse_id in video and video.endswith(file_extension) and os.stat(ROOT_FOLDER+folder+'/'+video).st_size)]\n",
    "    preexposure_videos.sort()\n",
    "    \n",
    "    for folder in postcond_folders:\n",
    "        postcond_videos += [video+'/'+folder for video in os.listdir(ROOT_FOLDER+folder) if (mouse_id in video and video.endswith(file_extension) and os.stat(ROOT_FOLDER+folder+'/'+video).st_size)]\n",
    "    postcond_videos.sort()\n",
    "    \n",
    "    for folder in postext_folders:\n",
    "        postext_videos += [video+'/'+folder for video in os.listdir(ROOT_FOLDER+folder) if (mouse_id in video and video.endswith(file_extension) and os.stat(ROOT_FOLDER+folder+'/'+video).st_size)]\n",
    "    postext_videos.sort()\n",
    "    \n",
    "    for folder in postret_folders:\n",
    "        postret_videos += [video+'/'+folder for video in os.listdir(ROOT_FOLDER+folder) if (mouse_id in video and video.endswith(file_extension) and os.stat(ROOT_FOLDER+folder+'/'+video).st_size)]\n",
    "    postret_videos.sort()\n",
    "    \n",
    "    if ret:\n",
    "        return preexposure_videos, postcond_videos\n",
    "    else:\n",
    "        print(\"\\nPREEXPOSURE:\")\n",
    "        print(\"\\n\".join(preexposure_videos))\n",
    "        print(\"\\nPOSTCONDITIONING:\")\n",
    "        print(\"\\n\".join(postcond_videos))\n",
    "        print(\"\\nPOSTEXTINCTION:\")\n",
    "        print(\"\\n\".join(postext_videos))\n",
    "        print(\"\\nPOSTRETRIEVAL:\")\n",
    "        print(\"\\n\".join(postret_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a467ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_counter = {\n",
    "#     'under5':0,\n",
    "#     '5-10':0,\n",
    "#     '10-15':0,\n",
    "#     '15-20':0,\n",
    "#     '20-25':0,\n",
    "#     '25-30':0,\n",
    "#     '30-35':0,\n",
    "#     '35-40':0,\n",
    "#     '40-45':0,\n",
    "#     '45-50':0,\n",
    "#     '50-55':0,\n",
    "#     '55-60':0,\n",
    "#     '60-90':0,\n",
    "#     '90-450':0,\n",
    "#     '450-1800':0,\n",
    "#     '1800-4500':0,\n",
    "#     'over4500':0,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_repeat_vocab(repeat_number):\n",
    "#     # > '0-5'\n",
    "#     # > '5-15'\n",
    "#     # > '15-45'\n",
    "#     # > '45-90'\n",
    "#     # > '90-450'\n",
    "#     # > '450-1800'\n",
    "#     # > '1800-4500'\n",
    "#     # > 'over4500'\n",
    "#     if repeat_number > 0 and repeat_number <= 5:\n",
    "#         box = \"under5\"\n",
    "#     elif repeat_number > 5 and repeat_number <= 10:\n",
    "#         box = \"5-10\"\n",
    "#     elif repeat_number > 10 and repeat_number <= 15:\n",
    "#         box = \"10-15\"\n",
    "#     elif repeat_number > 15 and repeat_number <= 20:\n",
    "#         box = \"15-20\"\n",
    "#     elif repeat_number > 20 and repeat_number <= 25:\n",
    "#         box = \"20-25\"\n",
    "#     elif repeat_number > 25 and repeat_number <= 30:\n",
    "#         box = \"25-30\"\n",
    "#     elif repeat_number > 30 and repeat_number <= 35:\n",
    "#         box = \"30-35\"\n",
    "#     elif repeat_number > 35 and repeat_number <= 40:\n",
    "#         box = \"35-40\"\n",
    "#     elif repeat_number > 40 and repeat_number <= 45:\n",
    "#         box = \"40-45\"\n",
    "#     elif repeat_number > 45 and repeat_number <= 50:\n",
    "#         box = \"45-50\"\n",
    "#     elif repeat_number > 50 and repeat_number <= 55:\n",
    "#         box = \"50-55\"\n",
    "#     elif repeat_number > 55 and repeat_number <= 60:\n",
    "#         box = \"55-60\"\n",
    "#     elif repeat_number > 60 and repeat_number <= 90:\n",
    "#         box = \"60-90\"\n",
    "#     elif repeat_number > 90 and repeat_number <= 450:\n",
    "#         box = \"90-450\"\n",
    "#     elif repeat_number > 450 and repeat_number <= 1800:\n",
    "#         box = \"450-1800\"\n",
    "#     elif repeat_number > 1800 and repeat_number <= 4500:\n",
    "#         box = \"1800-4500\"\n",
    "#     elif repeat_number > 4500:\n",
    "#         box = \"over4500\"\n",
    "#     else:\n",
    "#         raise ValueError('A very specific bad thing happened.')\n",
    "#     time_counter[box] += 1\n",
    "#     return box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f148a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleaning_actions(action_list):\n",
    "#     WINDOW_SIZE = 3 # Each side\n",
    "    \n",
    "#     index = WINDOW_SIZE\n",
    "#     action_length = len(action_list)\n",
    "    \n",
    "#     while index < (action_length - WINDOW_SIZE - 1):\n",
    "#         current_action = action_list[index]\n",
    "#         window = action_list[index - WINDOW_SIZE : index + WINDOW_SIZE + 1]\n",
    "#         window_elements = collections.Counter(window)\n",
    "#         most_common_action = window_elements.most_common(1)[0][0]\n",
    "#         if action_list[index] != most_common_action:\n",
    "#             action_list[index] = most_common_action\n",
    "#         index += 1\n",
    "#     return action_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eec4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_language_from_labels(action_list, class_label):\n",
    "#     SENTENCE_LENGTH = 900 # in terms of frames\n",
    "    \n",
    "#     start_index = 0\n",
    "#     action_length = len(action_list)\n",
    "    \n",
    "#     language = []\n",
    "# #     label = []\n",
    "#     sentence = \"\"\n",
    "    \n",
    "#     current_sentence_length = 0\n",
    "#     while start_index < action_length:\n",
    "        \n",
    "#         current_action = action_list[start_index]\n",
    "#         current_action_label = BEH_LABELS[current_action]\n",
    "        \n",
    "#         next_action_idx = start_index + 1\n",
    "        \n",
    "#         while next_action_idx < action_length and current_action == action_list[next_action_idx]:\n",
    "#             next_action_idx  += 1\n",
    "#         repeats = next_action_idx - start_index\n",
    "        \n",
    "#         if current_sentence_length == 0:\n",
    "#             sentence = \"\"\n",
    "            \n",
    "#         current_sentence_length += repeats\n",
    "#         start_index = next_action_idx\n",
    "        \n",
    "#         sentence += str(current_action_label)+\" \"+get_repeat_vocab(repeats)+\" \"\n",
    "        \n",
    "#         if current_sentence_length >= SENTENCE_LENGTH:\n",
    "#             language.append(sentence)\n",
    "# #             label.append(class_label)\n",
    "#             current_sentence_length = 0\n",
    "            \n",
    "#     if sentence != \"\":\n",
    "#         language.append(sentence)\n",
    "# #         label.append(class_label)\n",
    "#     return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_label(filename):\n",
    "#     if \"exp\" in filename:\n",
    "#         return 0\n",
    "    \n",
    "#     if \"cond\" in filename:\n",
    "#         return 1\n",
    "    \n",
    "#     if \"xt\" in filename:\n",
    "#         return 2\n",
    "    \n",
    "#     if \"tr\" in filename:\n",
    "#         return 3\n",
    "#     else:\n",
    "#         raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90aad1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Storing as Individual Action Files\n",
    "\n",
    "# folder_counter = 1\n",
    "# for folder in folder_list:\n",
    "#     print(\"\\nFOLDER:\", str(folder_counter)+\"/\"+str(len(folder_list)))\n",
    "#     folder_counter += 1\n",
    "#     action_files = os.listdir(ACTION_FOLDER + folder)\n",
    "#     csv_counter = 1\n",
    "#     for csv_name in action_files:\n",
    "#         actions = pd.read_csv(ACTION_FOLDER + folder + '/' + csv_name)\n",
    "#         print(\"\\n\"+str(csv_counter)+\"/\"+str(len(action_files))+\" Reading CSV:\", csv_name)\n",
    "#         csv_counter += 1\n",
    "#         current_actions = []\n",
    "#         for idx in range(len(actions)):\n",
    "#             current_actions.append(actions.iloc[idx][1])\n",
    "#         start = time.time()\n",
    "#         cleaned_actions = cleaning_actions(current_actions)\n",
    "#         cleaning = time.time()\n",
    "#         condensed_actions = generate_language_from_labels(cleaned_actions, get_label(folder))\n",
    "#         condensing = time.time()\n",
    "#         save_name = csv_name.split('.csv')[0] + \".npy\"\n",
    "#         saving = time.time()\n",
    "#         try:\n",
    "#             np.save(\"CondensedActions/\"+folder+\"/\"+save_name, condensed_actions)\n",
    "#         except:\n",
    "#             os.mkdir(\"CondensedActions/\"+folder)\n",
    "#             np.save(\"CondensedActions/\"+folder+\"/\"+save_name, condensed_actions)\n",
    "#         print(\"Actions for \"+folder+\"/\"+csv_name+\" processed. Label:\", get_label(folder))\n",
    "#         print(\"Times:: Cleaning:\", cleaning-start, \" Condensing:\", condensing-cleaning, \"Saving:\", saving-condensing, \" Total:\", saving-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d41477",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Storing as Folders\n",
    "\n",
    "# folder_counter = 1\n",
    "\n",
    "# for folder in condensed_action_folders:\n",
    "#     all_sequences = np.array([])\n",
    "#     all_labels = np.array([])\n",
    "#     print(\"\\nFOLDER:\", str(folder_counter)+\"/\"+str(len(condensed_action_folders)))\n",
    "#     folder_counter += 1\n",
    "#     action_files = os.listdir(CONDENSED_ACTION_FOLDER + folder)\n",
    "#     csv_counter = 1\n",
    "#     start = time.time()\n",
    "#     print(folder)\n",
    "#     for csv_name in action_files:\n",
    "#         actions = np.load(CONDENSED_ACTION_FOLDER + folder + '/' + csv_name)\n",
    "#         csv_counter += 1\n",
    "#         all_sequences = np.append(all_sequences, actions)\n",
    "#         all_labels = np.append(all_labels, [get_label(folder)]*len(actions))\n",
    "#         if len(all_sequences) != len(all_labels):\n",
    "#             raise ValueError\n",
    "#     print(\"Append Time:\", time.time()-start)\n",
    "#     np.save(\"./FolderWise/Sequences_\"+folder, all_sequences)\n",
    "#     np.save(\"./FolderWise/Labels_\"+folder, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd994f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Reading folders and appending:\n",
    "\n",
    "all_sequences = np.array([])\n",
    "all_labels = np.array([])\n",
    "\n",
    "folder_counter = 0\n",
    "for folder in condensed_action_folders:\n",
    "    start = time.time()\n",
    "    folder_counter += 1\n",
    "    \n",
    "    sequences = np.load(\"./FolderWise/Sequences_\"+folder+\".npy\", allow_pickle=True)\n",
    "    labels = np.load(\"./FolderWise/Labels_\"+folder+\".npy\", allow_pickle=True)\n",
    "    \n",
    "    all_sequences = np.append(all_sequences, sequences)\n",
    "    all_labels = np.append(all_labels, labels)\n",
    "    \n",
    "    csv_counter = 1\n",
    "    print(\"\\nFOLDER:\", )\n",
    "    print(str(folder_counter)+\"/\"+str(len(condensed_action_folders)),\"Append Time:\", time.time()-start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8816ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_sequences), len(all_sequences) == len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = -1\n",
    "seq_lens = []\n",
    "for sequence in all_sequences:\n",
    "    l = len(sequence.split(' '))\n",
    "    seq_lens.append(l)\n",
    "    if l > max_length:\n",
    "        max_length = l\n",
    "print(\"Maximum sequence length:\", max_length)\n",
    "seq_lens = np.array(seq_lens)\n",
    "print(\"Average:\", np.mean(seq_lens))\n",
    "print(\"Standard Deviation:\", np.std(seq_lens))\n",
    "\n",
    "# To ensure we don't miss anything, we will use 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2aeca9",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c7732e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\r\n",
      "[UNK]\r\n",
      "[CLS]\r\n",
      "[SEP]\r\n",
      "[MASK]\r\n",
      "under5\r\n",
      "5-10\r\n",
      "10-15\r\n",
      "15-20\r\n",
      "20-25\r\n",
      "25-30\r\n",
      "30-35\r\n",
      "35-40\r\n",
      "40-45\r\n",
      "45-50\r\n",
      "50-55\r\n",
      "55-60\r\n",
      "60-90\r\n",
      "90-450\r\n",
      "450-1800\r\n",
      "1800-4500\r\n",
      "over4500\r\n",
      "drink\r\n",
      "eat\r\n",
      "groom\r\n",
      "hang\r\n",
      "sniff\r\n",
      "rear\r\n",
      "rest\r\n",
      "walk\r\n",
      "nibble"
     ]
    }
   ],
   "source": [
    "!cat nih_vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# create a python generator to dynamically load the data\n",
    "def batch_iterator(batch_size=100):\n",
    "    for i in tqdm(range(0, len(raw_text), batch_size)):\n",
    "        print(\"Batch:\", raw_text[i : i + batch_size])\n",
    "        yield raw_datasets[i : i + batch_size]\n",
    "\n",
    "tokenizer = BertTokenizer(\"nih_vocab.txt\", do_basic_tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a44cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = 'sniff 20-25 groom under5 sniff 5-10 groom 10-15 sniff 40-45 groom 20-25 sniff 5-10 groom 20-25 sniff under5 groom 50-55 sniff 90-450 groom 20-25 sniff 60-90 groom 90-450 sniff 15-20 groom 30-35 sniff 55-60 groom 60-90 sniff 60-90 nibble 10-15 sniff 15-20 nibble 45-50 sniff 50-55'\n",
    "bert_input = tokenizer(example_text,padding='max_length', max_length = 512, \n",
    "                       truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c792472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rand_sentence():\n",
    "    '''\n",
    "    Displays the tokens and respective IDs of a random text sample\n",
    "    '''\n",
    "    index = random.randint(0, len(all_sequences)-1)\n",
    "    table = np.array([tokenizer.tokenize(all_sequences[index]), \n",
    "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(all_sequences[index]))]).T\n",
    "    print(tabulate(table,\n",
    "                 headers = ['Tokens', 'Token IDs'],\n",
    "                 tablefmt = 'fancy_grid'))\n",
    "\n",
    "print_rand_sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e78a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into required format\n",
    "dataset= LineByLineTextDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    file_path = 'all_30s_sequences.txt',\n",
    "    block_size = 256\n",
    ")\n",
    "print('No. of lines: ', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce259c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining configuration of BERT for pretraining\n",
    "config = BertConfig(\n",
    "    vocab_size=31,\n",
    "    hidden_size=768, \n",
    "    num_hidden_layers=6, \n",
    "    num_attention_heads=12,\n",
    "    max_position_embeddings=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM(config)\n",
    "print('No of parameters: ', model.num_parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a85def",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d31d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining training configuration\\\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='.',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=7,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Perfrom pre-training and save the model\n",
    "trainer.train()\n",
    "trainer.save_model('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d56978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = BertForSequenceClassification(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "707ac8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(256, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8805790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
